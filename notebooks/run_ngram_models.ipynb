{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692ce30b",
   "metadata": {},
   "source": [
    "# example code for training and inference on ngram models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f8b9b",
   "metadata": {},
   "source": [
    "## define imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e593d897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# get project root for file paths and add project root to python path so imports work from notebooks folder\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from ngram import *\n",
    "\n",
    "# constants\n",
    "DATA_PATH = str(PROJECT_ROOT / \"data\" / \"scrape_results_2019_2022.csv\")  # path where data is located"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8f421",
   "metadata": {},
   "source": [
    "## load and peek data\n",
    "\n",
    "data is expected to be in format:\n",
    "<pre>\n",
    "[\n",
    "    [0, 1, ...], # list expected to contain 82 entries of 0's or 1's (1's representing wins and 0's losses) \n",
    "    [0, 0, 1, ...],\n",
    "    # can contain as many lists as needed\n",
    "]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e9d2f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mock data sample: [\n",
      "[1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1]\n",
      "[1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0]\n",
      "...]\n",
      "\n",
      "real data sample: [\n",
      "[1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1]\n",
      "len: 64\n",
      "[0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0]\n",
      "len: 63\n",
      "...]\n",
      "all data 90\n"
     ]
    }
   ],
   "source": [
    "# can look up specific team and season with raw_data\n",
    "real_raw_data = read_data(DATA_PATH)\n",
    "\n",
    "# real_data is processed for training\n",
    "real_data = list(real_raw_data.values())\n",
    "mock_data = [generate_season_data() for _ in range(50)] # generate random data for 50 season long records   \n",
    "\n",
    "print(\"mock data sample: [\")\n",
    "for season in mock_data[:2]:\n",
    "    print(season)\n",
    "print(\"...]\")\n",
    "print()\n",
    "\n",
    "print(\"real data sample: [\")\n",
    "for season in real_data[:2]:\n",
    "    print(season)\n",
    "    print(\"len:\", len(season))\n",
    "print(\"...]\")\n",
    "\n",
    "print(\"all data\", len(real_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde6a952",
   "metadata": {},
   "source": [
    "## train models using mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54657e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing team records: 100%|██████████| 90/90 [00:00<00:00, 163343.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed training unigram model\n",
      "unigram model cpts {'initial': {1: 0.5, 0: 0.5}}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing team records: 100%|██████████| 90/90 [00:00<00:00, 97315.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed training bigram model\n",
      "bigram model cpts {'initial': {1: 0.5, 0: 0.5}, 'transition': {1: {1: 0.5390793945010812, 0: 0.4609206054989188}, 0: {1: 0.4609206054989188, 0: 0.5390793945010812}}}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing team records: 100%|██████████| 90/90 [00:00<00:00, 75770.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed training trigram model\n",
      "trigram model cpts {'initial': {'<start>': {1: 0.5, 0: 0.5}, 1: {1: 0.6, 0: 0.4}, 0: {1: 0.4, 0: 0.6}}, 'transition': {1: {1: {1: 0.574245939675174, 0: 0.425754060324826}, 0: {1: 0.5023825731790333, 0: 0.49761742682096666}}, 0: {1: {1: 0.49591280653950953, 0: 0.5040871934604905}, 0: {1: 0.4271619268717353, 0: 0.5728380731282646}}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unigram_model = train_unigram(real_data)\n",
    "print(\"unigram model cpts\", unigram_model)\n",
    "print()\n",
    "\n",
    "bigram_model = train_bigram(real_data)\n",
    "print(\"bigram model cpts\", bigram_model)\n",
    "print()\n",
    "\n",
    "trigram_model = train_trigram(real_data)\n",
    "print(\"trigram model cpts\", trigram_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efbfbd6",
   "metadata": {},
   "source": [
    "## infer and measure performance using models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f300250a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "averages: {'unigram': 0.5018167218709223, 'bigram': 0.4977416440831075, 'trigram': 0.5160744755595703}\n",
      "wrote results to team_season_ngram_results.txt\n"
     ]
    }
   ],
   "source": [
    "# mock actual season\n",
    "# actual_season = generate_season_data() # random generates season data\n",
    "# get actual season from real data\n",
    "SEASON = \"2021\"\n",
    "TEAMS = [\n",
    "    \"ATL\", \"BOS\", \"BRK\", \"CHI\", \"CHO\", \"CLE\", \"DAL\", \"DEN\", \"DET\", \"GSW\",\n",
    "    \"HOU\", \"IND\", \"LAC\", \"LAL\", \"MEM\", \"MIA\", \"MIL\", \"MIN\", \"NOP\", \"NYK\",\n",
    "    \"OKC\", \"ORL\", \"PHI\", \"PHO\", \"POR\", \"SAC\", \"SAS\", \"TOR\", \"UTA\", \"WAS\"\n",
    "]\n",
    "\n",
    "def run_for_team_season(team, season, f):\n",
    "    actual_season = real_raw_data[team + season]\n",
    "    f.write(f\"{team} {season} actual season {actual_season}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    # print(f\"{team} {season} actual season\", actual_season)\n",
    "    # print()\n",
    "\n",
    "    def infer_and_eval_model(model_type, model):\n",
    "        if model_type == \"unigram\":\n",
    "            predictions = infer_unigram_season(model)\n",
    "        elif model_type == \"bigram\":\n",
    "            predictions = infer_bigram_season(model)\n",
    "        elif model_type == \"trigram\":\n",
    "            predictions = infer_trigram_season(model)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model type\")\n",
    "        \n",
    "        f.write(f\"{model_type} predictions {predictions}\\n\")\n",
    "        # print(f\"{model_type} predictions\", predictions)\n",
    "        \n",
    "        accuracy = sequence_accuracy(actual_season, predictions)\n",
    "        f.write(f\"{model_type} model accuracy: {accuracy}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        # print(f\"{model_type} model accuracy:\", accuracy)\n",
    "        # print()\n",
    "        return accuracy\n",
    "\n",
    "    models = [(\"unigram\", unigram_model), (\"bigram\", bigram_model), (\"trigram\", trigram_model)]\n",
    "    model_stats = {\n",
    "        \"unigram\": {},\n",
    "        \"bigram\": {},\n",
    "        \"trigram\": {}\n",
    "    }\n",
    "    for model_type, model in models:\n",
    "        results = infer_and_eval_model(model_type, model)\n",
    "        model_stats[model_type] = results\n",
    "\n",
    "    return model_stats\n",
    "\n",
    "team_stats = {}\n",
    "with open(\"team_season_ngram_results.txt\", \"w\") as f:\n",
    "    for team in TEAMS:\n",
    "        team_stats[team] = run_for_team_season(team, SEASON, f)\n",
    "\n",
    "averages = {\n",
    "    \"unigram\": 0,\n",
    "    \"bigram\": 0,\n",
    "    \"trigram\": 0\n",
    "}\n",
    "for team in TEAMS:\n",
    "    for model_type in averages.keys():\n",
    "        averages[model_type] += team_stats[team][model_type]\n",
    "\n",
    "for key in averages.keys():\n",
    "    averages[key] /= len(TEAMS)\n",
    "\n",
    "print(\"averages:\", averages)\n",
    "\n",
    "print(\"wrote results to team_season_ngram_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59b9538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
